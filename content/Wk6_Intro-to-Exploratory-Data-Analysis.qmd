---
title: Introduction to Exploratory Data Analysis
author: Dong Ye
toc: true
number-sections: true
highlight-style: pygments
project:
  type: website
  output-dir: docs
website:
  title: Introduction to Exploratory Data Analysis
  navbar:
    left:
      text: Home
format:
  html:
    theme: cosmo
    toc: true
    code-fold: true
    code-tools: true
    html-math-method: katex
  pdf:
    documentclass: scrreport
    geometry:
      - top=30mm
      - left=20mm
bibliography: references.bib
jupyter: python3
---

Exploratory Data Analysis (EDA) is an approach to analyzing and summarizing datasets, with the primary goal of discovering patterns, relationships, and insights that may not be immediately apparent. EDA is often the first step in the data analysis process and is crucial in generating hypotheses and developing a deeper understanding of the data before building predictive models or making conclusions.

In EDA, you may use a variety of techniques such as plotting histograms, scatter plots, box plots, and other graphical representations to visualize the distribution and relationships between variables. You might also use summary statistics such as mean, median, and standard deviation to describe the central tendency and dispersion of the data.

The main objective of EDA is to understand the underlying structure of the data, identify outliers, detect anomalies, and gain insights that can inform the development of more advanced models and methods.

Several questions come to mind when we come across a new dataset.  The below list shed light on some of these questions:-

•	What is the distribution of the dataset?

•	Are there any missing numerical values, outliers or anomalies in the dataset?

•	What are the underlying assumptions in the dataset?

•	Whether there exists relationships between variables in the dataset?

•	How to be sure that our dataset is ready for input in a machine learning algorithm?

•	How to select the most suitable algorithm for a given dataset?

## Objectives of EDA {#sec-objectives}


The objectives of the EDA are as follows:-

i.	To get an overview of the distribution of the dataset.

ii.	Check for missing numerical values, outliers or other anomalies in the dataset.

iii. Discover patterns and relationships between variables in the dataset.

iv.	Check the underlying assumptions in the dataset.

## Step 0: Import and Read Data {#sec-stpe0}

        Install the libraries.
        Import the libararies and Modules.
        Import and read the dataset(s).

**Import the required Python libraries**


We need two Python libraries for exploratory data analysis – NumPy and Pandas.

• NumPy – NumPy is the fundamental Python library for scientific computing. It adds support for large and multi-dimensional arrays and matrices. It also supports large collection of high-level mathematical functions to operate on these arrays.

• Pandas - Pandas is a software library for Python programming language which provide tools for data manipulation and analysis tasks. It will enable us to manipulate numerical tables and time series using data structures and operations.

We need two more libraries for data visualization purpose. These are Seaborn and Matplotlib.

• Seaborn - Seaborn is a Python data visualization library based on Matplotlib. It provides a high level interface for drawing attractive and informative statistical graphics.

• Matplotlib - Matplotlib is the core data visualization library of Python programming language. It provides an object-oriented API for embedding plots into applications.


We need to import these libraries before we actually start using them. We can import them with their usual shorthand notation as follows:

```{python}
# ignore the warnings

import warnings
warnings.simplefilter(action = "ignore", category = FutureWarning)
```

This code is used to suppress warnings generated by the Python interpreter. The warnings module provides a way to raise warnings and to control the reporting of warnings. 

The simplefilter method is used to ignore warnings that have a certain category. In this case, the FutureWarning category is being ignored. This is often used when you are aware of a deprecation in a library you are using and want to temporarily suppress the warning messages.

```{python}
#| _cell_guid: b1076dfc-b9ad-4769-8c92-a6c4dae69d19
#| _uuid: 8f2839f25d086af736a60e9eeb907d3b93b6e0e5
#| papermill: {duration: 0.950081, end_time: '2021-12-31T22:25:24.521694', exception: false, start_time: '2021-12-31T22:25:23.571613', status: completed}
#| tags: []
import pandas as pd
import numpy as np
import matplotlib.pylab as plt
import seaborn as sns
%matplotlib inline
```

The %matplotlib inline is a Jupyter magic command that is used to display the plots generated by Matplotlib in the output cells of the Jupyter notebook. 

It is a convenience command that allows you to quickly visualize the plots within the notebook, without having to manually call plt.show(). The %matplotlib inline command must be executed before any plotting is done in order to display the plots within the notebook. 

Note that this command is specific to Jupyter notebooks and is not a standard part of the Python programming language.

```{python}
sns.set(style="whitegrid")
```

The sns.set(style="whitegrid") line is a command from the Seaborn library, a popular data visualization library based on Matplotlib. 

The sns.set() function is used to set the style of the plots generated by Seaborn. The style parameter takes a string argument that specifies the style to use. 

In this case, the argument is "whitegrid", which sets the style of the plots to a white background with an overlaid grid. Seaborn provides several pre-defined styles that can be used to quickly change the look and feel of your plots. 

By setting the style using the sns.set() function, you can ensure a consistent look and feel for all of your plots in a project.

```{python}
pip install scikit-learn
```

```{python}
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import ExtraTreesClassifier
```

```{python}
#| papermill: {duration: 0.070227, end_time: '2021-12-31T22:25:24.611069', exception: false, start_time: '2021-12-31T22:25:24.540842', status: completed}
#| tags: []
df = pd.read_csv('https://raw.githubusercontent.com/Dong2Yo/Dataset/main/coaster.csv')
```

- Dataframe `shape`
- `head` and `tail`
- `dtypes`
- `describe`

```{python}
#| papermill: {duration: 0.029721, end_time: '2021-12-31T22:25:24.716333', exception: false, start_time: '2021-12-31T22:25:24.686612', status: completed}
#| tags: []
df.shape
```

```{python}
#| papermill: {duration: 0.068471, end_time: '2021-12-31T22:25:24.805357', exception: false, start_time: '2021-12-31T22:25:24.736886', status: completed}
#| tags: []
df.head()
```

```{python}
#| papermill: {duration: 0.028414, end_time: '2021-12-31T22:25:24.854178', exception: false, start_time: '2021-12-31T22:25:24.825764', status: completed}
#| tags: []
df.columns
```

```{python}
#| papermill: {duration: 0.030131, end_time: '2021-12-31T22:25:24.904578', exception: false, start_time: '2021-12-31T22:25:24.874447', status: completed}
#| tags: []
df.dtypes
```

```{python}
#| papermill: {duration: 0.065671, end_time: '2021-12-31T22:25:24.991200', exception: false, start_time: '2021-12-31T22:25:24.925529', status: completed}
#| tags: []
df.describe()
```

```{python}
df.info()
```

## Step 1: Conduct Data Cleaning {#sec-step1}

The first step in EDA is to clean and pre-process the data. This involves checking for missing values, correcting inconsistent data, handling outliers, and transforming variables as necessary.

Some of the questions we will ask ourselves are

    * Are there any useless or redundant variables?
    * Are there any duplicate columns?
    * Does the nomenclature make sense?
    * Are there any new variables we want to create?

During this step, we might perform the following tasks:

    1. Check for missing values: We would look for any rows in the dataset that contain missing values and determine how to handle them. For example, we might decide to remove rows with missing values or impute missing values using the mean or median of the data.

    2. Correct inconsistent data: We would check for any inconsistencies in the data, such as incorrect data types or incorrect values. For example, we might find that the number of bedrooms or bathrooms is recorded as a string instead of a numerical value. We would need to correct these inconsistencies to make the data suitable for analysis.

    3. Handle outliers: We would look for any outliers in the data and determine how to handle them. For example, we might decide to remove any homes with extremely high or low sale prices, as these homes may not be representative of the typical market.

    4. Transform variables: We might also decide to transform some of the variables, such as converting square footage from square feet to square meters. This could help to standardize the data and make it easier to compare variables.

These are just a few examples of tasks that might be performed during the data cleaning step of EDA. The specific tasks performed will depend on the dataset and the goals of the analysis.

```{python}
# Check for missing values
print(df.isnull().sum())
```

```{python}
#| papermill: {duration: 0.030091, end_time: '2021-12-31T22:25:25.093436', exception: false, start_time: '2021-12-31T22:25:25.063345', status: completed}
#| tags: []
# Example of dropping columns
# df.drop(['Opening date'], axis=1)
```

```{python}
#| papermill: {duration: 0.035435, end_time: '2021-12-31T22:25:25.152317', exception: false, start_time: '2021-12-31T22:25:25.116882', status: completed}
#| tags: []
df2 = df[['coaster_name',
    # 'Length', 'Speed',
    'Location', 'Status',
    # 'Opening date',
    #   'Type',
    'Manufacturer',
#     'Height restriction', 'Model', 'Height',
#        'Inversions', 'Lift/launch system', 'Cost', 'Trains', 'Park section',
#        'Duration', 'Capacity', 'G-force', 'Designer', 'Max vertical angle',
#        'Drop', 'Soft opening date', 'Fast Lane available', 'Replaced',
#        'Track layout', 'Fastrack available', 'Soft opening date.1',
#        'Closing date',
#     'Opened', 
    # 'Replaced by', 'Website',
#        'Flash Pass Available', 'Must transfer from wheelchair', 'Theme',
#        'Single rider line available', 'Restraint Style',
#        'Flash Pass available', 'Acceleration', 'Restraints', 'Name',
       'year_introduced',
        'latitude', 'longitude',
    'Type_Main',
       'opening_date_clean',
    #'speed1', 'speed2', 'speed1_value', 'speed1_unit',
       'speed_mph', 
    #'height_value', 'height_unit',
    'height_ft',
       'Inversions_clean', 'Gforce_clean']].copy()
```

converting a column in a pandas DataFrame from a string representation of a date/time to a pandas datetime object.

To find the data type of a single variable in a pandas DataFrame, you can use the dtype attribute of the corresponding column. For example, if you have a DataFrame df and you want to find the data type of the column "column_name", you can do the following:

```{python}
#| papermill: {duration: 0.030131, end_time: '2021-12-31T22:25:24.904578', exception: false, start_time: '2021-12-31T22:25:24.874447', status: completed}
#| tags: []
data_type = df2['opening_date_clean'].dtype
print(data_type)
```

This will print the data type of the column "column_name" in the DataFrame df. The data type could be one of several types, including int64, float64, object, or datetime64, among others. You can use this information to determine if the data in a particular column needs to be transformed or if there are any inconsistencies in the data that need to be corrected.

```{python}
df2['opening_date_clean'] = pd.to_datetime(df2['opening_date_clean'])
```

```{python}
#| papermill: {duration: 0.030131, end_time: '2021-12-31T22:25:24.904578', exception: false, start_time: '2021-12-31T22:25:24.874447', status: completed}
#| tags: []
print(data_type)
```

```{python}
# Rename our columns
df3 = df2.rename(columns={'coaster_name':'Coaster_Name',
                   'year_introduced':'Year_Introduced',
                   'opening_date_clean':'Opening_Date',
                   'speed_mph':'Speed_mph',
                   'height_ft':'Height_ft',
                   'Inversions_clean':'Inversions',
                   'Gforce_clean':'Gforce'})
```

```{python}
# Count the number of missing values (NaN values) in each column of a pandas DataFrame
df3.isna().sum()
```

```{python}
# Handle missing values by imputing the mean value for each column
# works with numbers
# df = df.fillna(df.mean())
```

The df.duplicated() method returns a boolean array indicating whether each row in the DataFrame is a duplicate of a previous row. True indicates that a row is a duplicate, and False indicates that a row is not a duplicate.

The df.loc method is then used to filter the original DataFrame df based on the boolean array returned by df.duplicated(). Specifically, df.loc[df.duplicated()] returns the rows where the value is True, meaning that those rows are duplicates.

The output of this code will be a new DataFrame containing only the rows that are duplicates in the original DataFrame. This information can be useful in identifying and removing duplicates from the data to ensure the accuracy of your analysis.

```{python}
df3.loc[df3.duplicated()]
```

```{python}
# Check for duplicate coaster name
df3.loc[df3.duplicated(subset=['Coaster_Name'])].head()
```

```{python}
# Checking an example duplicate
df3.query('Coaster_Name == "Crystal Beach Cyclone"')
```

```{python}
df3.columns
```

```{python}
df4 = df3.loc[~df3.duplicated(subset=['Coaster_Name','Location','Opening_Date'])] \
    .reset_index(drop=True).copy()
```

```{python}
df4.head()
```

```{python}
df4.isna().sum()
```

```{python}
df4.loc[df4.duplicated()]
```

```{python}
df5 = df4.drop_duplicates(subset=["Coaster_Name"], inplace= False)
```

In this example, df is the DataFrame, subset is a parameter that takes a list of column names to consider when identifying duplicates, and inplace=True specifies that the changes should be made to the original DataFrame df rather than returning a new DataFrame.

Note that drop_duplicates by default keeps the first occurrence of each duplicate and drops the remaining duplicates. If you want to keep the last occurrence of each duplicate instead, you can use the keep='last' argument.

```{python}
# check
df5.head()
```

## Step 2: Univariate Analysis {#sec-step2}

This involves examining each variable in isolation to understand its distribution, central tendency, and spread. This can be done using techniques such as histograms, density plots, and box plots.

**Univariate distribution**

In univariate distribution, there is only one variable under consideration. It is the simplest form of analysis because only one quantity changes. It does not deal with causes or relationships. 

The main purpose of the analysis is to describe the data and find patterns that exist within it. We can describe patterns found in univariate data using central tendency (mean, median and mode) and dispersion (range, variance, standard deviation, maximum and minimum values and interquartile range). 

We can visualize the univariate data using various types of charts and graphs. These are frequency distribution tables, histograms, bar charts, pie charts and frequency polygons.

In Python, using the pandas library, univariate analysis can be performed using several built-in functions and methods.

    1. Descriptive statistics: Use the describe method to get a summary of the central tendency, dispersion, and shape of the distribution of the data. 

```{python}
df5['Year_Introduced'].describe()
```

2. Frequency distribution: Use the value_counts method to get a frequency distribution of the unique values in a column. 

```{python}
#| papermill: {duration: 0.03705, end_time: '2021-12-31T22:25:26.622753', exception: false, start_time: '2021-12-31T22:25:26.585703', status: completed}
#| tags: []
df5['Type_Main'].value_counts()
```

```{python}
#| papermill: {duration: 0.03309, end_time: '2021-12-31T22:25:25.709352', exception: false, start_time: '2021-12-31T22:25:25.676262', status: completed}
#| tags: []
df5['Year_Introduced'].value_counts()
```

```{python}
#| label: fig-simple
#| fig-cap: Top 10 Years
#| papermill: {duration: 0.283261, end_time: '2021-12-31T22:25:26.016166', exception: false, start_time: '2021-12-31T22:25:25.732905', status: completed}
#| tags: []
ax = df5['Year_Introduced'].value_counts() \
    .head(10) \
    .plot(kind='bar', title='Top 10 Years Coasters Introduced')
ax.set_xlabel('Year Introduced')
ax.set_ylabel('Count')
```

    3. Histogram: Use the hist method to visualize the distribution of the data in a column. 

```{python}
df5['Year_Introduced'].hist()
```

```{python}
#| papermill: {duration: 0.294699, end_time: '2021-12-31T22:25:26.334934', exception: false, start_time: '2021-12-31T22:25:26.040235', status: completed}
#| tags: []
ax = df5['Speed_mph'].plot(kind='hist',
                          bins=20,
                          title='Coaster Speed (mph)')
ax.set_xlabel('Speed (mph)')
```

    4. Density Plot: Use the density method to create a density plot, which is a smoothed version of a histogram

```{python}
df5['Speed_mph'].plot.density()
```

```{python}
#| papermill: {duration: 0.199427, end_time: '2021-12-31T22:25:26.559519', exception: false, start_time: '2021-12-31T22:25:26.360092', status: completed}
#| tags: []
ax = df5['Speed_mph'].plot(kind='kde',
                          title='Coaster Speed (mph)')
ax.set_xlabel('Speed (mph)')
```

    5. Box Plot: Use the boxplot method to visualize the distribution of the data in a column and identify any outliers.

```{python}
df5.boxplot(column='Speed_mph')
```

    6. Violin Plot: Use the violinplot method to visualize the distribution of the data in a column and identify any outliers.

```{python}
sns.violinplot(x=df5['Speed_mph'])
```

## Step 3: Bivariate Analysis {#sec-step3}

**Bivariate distribution**

This type of data distribution involves two different variables. The analysis of this type of data deals with causes and relationships and the analysis is done to find out the relationship among the two variables. A very common example of bivariate distribution is height and weight of a single person.

Bivariate analysis means the analysis of bivariate data. It is one of the simplest forms of statistical analysis, used to find out if there is a relationship between two sets of values. Thus bivariate data analysis involves comparisons, exploring relationships, finding causes and explanations. These variables are often plotted on X and Y axis on the graph for better understanding of data and one of these variables is independent while the other is dependent.

Common types of bivariate analysis include drawing scatter plot, regression analysis and finding correlation coefficients. A scatter plot is used to find out if there exists any relationship between two variables. Regression analysis is a statistical method for estimating the relationships between variables. Correlation coefficient analysis measures the strength and direction of a linear relationship between two variables on a scatter plot.

- Scatterplot
- Heatmap Correlation
- Pairplot
- Groupby comparisons

    1. Visualize the relationship: Plot a scatter plot, a hexbin plot, a line plot, or a bar plot to visualize the relationship between the two variables.

```{python}
#| papermill: {duration: 0.183301, end_time: '2021-12-31T22:25:26.884625', exception: false, start_time: '2021-12-31T22:25:26.701324', status: completed}
#| tags: []
df5.plot(kind='scatter',
        x='Speed_mph',
        y='Height_ft',
        title='Coaster Speed vs. Height')
plt.show()
```

```{python}
#| papermill: {duration: 0.295762, end_time: '2021-12-31T22:25:27.208343', exception: false, start_time: '2021-12-31T22:25:26.912581', status: completed}
#| tags: []
ax = sns.scatterplot(x='Speed_mph',
                y='Height_ft',
                hue='Year_Introduced',
                data=df5)
ax.set_title('Coaster Speed vs. Height')
plt.show()
```

```{python}
#| papermill: {duration: 6.429785, end_time: '2021-12-31T22:25:33.666958', exception: false, start_time: '2021-12-31T22:25:27.237173', status: completed}
#| tags: []
sns.pairplot(df5,
             vars=['Year_Introduced','Speed_mph',
                   'Height_ft','Inversions','Gforce'],
            hue='Type_Main')
plt.show()
```

    2. Calculate the correlation: Use the corr method to calculate the Pearson correlation coefficient, which measures the linear relationship between two variables. A value of +1 indicates a strong positive relationship, a value of -1 indicates a strong negative relationship, and a value of 0 indicates no relationship.

```{python}
# Calculate the correlation
correlation = df5["Year_Introduced"].corr(df5["Gforce"])
print("Correlation:", correlation)
```

```{python}
#| papermill: {duration: 0.053233, end_time: '2021-12-31T22:25:33.756533', exception: false, start_time: '2021-12-31T22:25:33.703300', status: completed}
#| tags: []
df5_corr = df5[['Year_Introduced','Speed_mph',
    'Height_ft','Inversions','Gforce']].dropna().corr()
df5_corr
```

```{python}
#| papermill: {duration: 0.305333, end_time: '2021-12-31T22:25:34.098497', exception: false, start_time: '2021-12-31T22:25:33.793164', status: completed}
#| tags: []
sns.heatmap(df5_corr, annot=True)
```

    3. Check for outliers: Use box plots, violin plots, or scatter plots to identify any outliers in the data. Outliers can greatly affect the correlation coefficient and need to be treated accordingly.

```{python}
# Check for outliers
sns.boxplot(x="Height_ft", data=df5)
```

```{python}
sns.boxplot(x="Year_Introduced", data=df5)
```

    4. Determine the type of relationship: Determine if the relationship is linear, non-linear, or non-existent. This can be done by visual inspection of the plots or by using statistical methods such as regression analysis.

```{python}
# Determine the type of relationship
sns.regplot(x="Height_ft", y="Year_Introduced", data=df5)
```

    5. Test for significance: Use hypothesis testing to determine if the correlation between the two variables is statistically significant. This can be done using a t-test or an ANOVA test.

```{python}
from scipy.stats import pearsonr
correlation, p_value = pearsonr(df5["Inversions"], df5["Year_Introduced"])
print("Correlation:", correlation, "P-value:", p_value)
```

The six steps are a general guide for conducting bivariate analysis, and the specific methods used may vary depending on the type of data, the research question, and the tools available. However, by following these steps, you can gain a deeper understanding of the relationship between two variables and make informed conclusions about the data.

    6. Draw conclusions: Based on the results of the bivariate analysis, draw conclusions about the relationship between the two variables and any potential causality between them.

```{python}
# Draw conclusions
if p_value < 0.05:
    print("There is a statistically significant relationship between Inversions and Years Introduced.")
else:
    print("There is not a statistically significant relationship between Inversions and Years Introduced.")
```

## Ask a Question about the data {#sec-q}
- Try to answer a question you have about the data using a plot or statistic.

What are the locations with the fastest roller coasters (minimum of 10)?

```{python}
#| papermill: {duration: 0.382299, end_time: '2021-12-31T22:25:34.597495', exception: false, start_time: '2021-12-31T22:25:34.215196', status: completed}
#| tags: []
ax = df5.query('Location != "Other"') \
    .groupby('Location')['Speed_mph'] \
    .agg(['mean','count']) \
    .query('count >= 10') \
    .sort_values('mean')['mean'] \
    .plot(kind='barh', figsize=(12, 5), title='Average Coast Speed by Location')
ax.set_xlabel('Average Coaster Speed')
plt.show()
```

## Step 4: Multivariate Analysis {#sec-step4}

**Multivariate distribution**

When the dataset involves three or more variables, it is categorized under multivariate distribution.  Multivariate analysis is used to study more complex sets of data. It is usually unsuitable for small sets of data.

Multivariate analysis helps to identify patterns, relationships, and interactions between multiple variables and provides a more comprehensive understanding of the data than univariate or bivariate analysis.

There are wide variety of analysis techniques to perform multivariate analysis. The choice of analysis techniques depends on the dataset and our goals to be achieved. 

Some examples of multivariate analysis techniques are additive tree, cluster analysis, correspondence analysis, factor analysis, MANOVA (multivariate analysis of variance), multidimensional scaling, multiple regression analysis, principal component analysis and redundancy analysis.

There are several useful multivariate graphical EDA techniques, which are used to look at the distribution of multivariate data. These are as follows:-

    Side-by-Side Boxplots

    Scatterplots

    Curve Fitting

    Heat Maps and 3-D Surface Plots

Here are the steps for conducting multivariate analysis:

    1. Identify the research question: Clearly define the research question you want to answer with the multivariate analysis.

    2. Prepare the data: Clean and transform the data as needed, and create a new dataset that includes all the variables of interest.

    3. Visualize the relationships: Use scatter plots, pairwise plots, or heat maps to visualize the relationships between all the variables.

    4. Perform dimension reduction: Use techniques such as principal component analysis (PCA) or singular value decomposition (SVD) to reduce the number of variables while retaining the maximum amount of information.

    5. Clustering: Use clustering algorithms such as k-means or hierarchical clustering to group similar observations based on the variables.

    6. Modeling: Use multivariate regression, logistic regression, or other statistical models to estimate the relationships between the variables and predict outcomes.

    7. Evaluate the models: Evaluate the performance of the models using metrics such as mean squared error (MSE), root mean squared error (RMSE), or accuracy.

    8. Draw conclusions: Based on the results of the analysis, draw conclusions about the relationships between the variables and any potential causality between them.

These steps are a general guide for conducting multivariate analysis, and the specific methods used may vary depending on the type of data, the research question, and the tools available. 

### Review Step 0-3

```{python}
# Load the data
df6 = pd.read_csv("https://raw.githubusercontent.com/Dong2Yo/Dataset/main/winequality.csv")
```

```{python}
df6.head()
```

```{python}
df6.tail()
```

```{python}
df6.shape
```

```{python}
df6.describe()
```

```{python}
df6.info()
```

```{python}
df6.dtypes
```

```{python}
df6.isna()
```

```{python}
df6.duplicated().sum()
```

```{python}
df6['fixed acidity'].describe()
```

```{python}
sns.histplot(df6['fixed acidity'])
```

```{python}
print(f"Skewness: {df6['fixed acidity'].skew()}")
print(f"Kurtosis: {df6['fixed acidity'].kurt()}")
```

From this information we see how the distribution:

    does not follow a normal curve
    show spikes
    has kurtosis and asymmetry values greater than 1

We do this for each variable, and we will have a pseudo-complete descriptive picture of their behavior.

We need this work to fully understand each variable, and unlocks the study of the relationship between variables.

Now the idea is to find interesting relationships that show the influence of one variable on the other, preferably on the target.

We can start exploring relationships with the help of Seaborn and pairplot.

```{python}
# Visualize the relationships
sns.pairplot(df6)
```

As you can see, pairplot displays all the variables against each other in a scatterplot. It is very useful for grasping the most important relationships without having to go through every single combination manually. 

Be warned though — it is computationally expensive to compute, so it is best suited for datasets with relatively low number of variables like this one.

The best way to understand the relationship between a numeric variable and a categorical variable is through a boxplot.

Let’s create a boxplot for sulphates, citric acid, and chlorides. Why these variables? Because visually they show slightly more marked segmentations for a given wine type. 

```{python}
sns.catplot(x="quality", y="sulphates", data=df6, kind="box", aspect=1.5)
plt.title("Boxplot for quality vs sulphates")
plt.show()
```

```{python}
sns.catplot(x="quality", y="citric acid", data=df6, kind="box", aspect=1.5)
plt.title("Boxplot for quality vs citric acid")
plt.show()
```

```{python}
sns.catplot(x="quality", y="chlorides", data=df6, kind="box", aspect=1.5)
plt.title("Boxplot for quality vs chlorides")
plt.show()
```

With Seaborn we can create a scatterplot and visualize which wine class a point belongs to. Just specify the hue parameter.

```{python}
sns.scatterplot(x="chlorides", y="citric acid", hue="quality", data=df6, palette="Dark2", s=80)
plt.title("Relationship between chlorides, citric acid and quality")
plt.show()
```


PCA stands for Principal Component Analysis, which is a widely used statistical method for dimensionality reduction. 

PCA is a technique that is used to transform a set of correlated variables into a set of uncorrelated variables, called principal components, which explain the maximum amount of variability in the data. The first principal component explains the most variability in the data, the second principal component explains the second most variability, and so on. By reducing the number of variables, PCA makes it easier to visualize and analyze large and complex datasets, and can also improve the performance of machine learning models.

**PCA only works with numerical data.**

To change the string values "white" and "red" in a column to integer values, we can use the map method in Python.

```{python}
df6['type'] = df6['type'].map({'white': 0, 'red': 1})
```

```{python}
df6.dtypes
```

```{python}
df6['type'] = df6['type'].astype(float)
df6['quality'] = df6['quality'].astype(float)
```

```{python}
df6.dtypes
```

**PCA does not accept missing values encoded as NaN natively.**

```{python}
missing = df6.isna().sum()
missing
```

```{python}
sns.heatmap(df6.isnull(), cbar=False)
plt.show()

#heatmap is not a good visual representation.
```

```{python}
# plot the missing value
missing.plot.bar()
plt.show()
```

Handling missing data is a common problem in data analysis, and there are several methods for addressing it. Here are a few common approaches:

    1. Drop missing values: This involves removing all rows or columns that contain missing values. This method is simple and straightforward, but can result in a loss of important information if too many values are missing.

    'df.dropna(axis=0, inplace=True)'

    2. Fill missing values: This involves replacing missing values with a value from the same column (such as the mean or median), or from another column.

    'df.fillna(df.mean(), inplace=True)'

    3. Interpolate missing values: This involves filling in missing values by estimating their values based on the values of other observations in the same column.

    'df.interpolate(inplace=True)'

    4. Impute missing values: This involves filling in missing values using statistical methods, such as linear regression or k-nearest neighbors.

    'from sklearn.impute import KNNImputer'
    'imputer = KNNImputer()'
    'df_imputed = imputer.fit_transform(df)''


The appropriate method for handling missing data depends on the type of data and the context of the problem. It's important to carefully consider the trade-off between keeping missing data and imputing it, and to evaluate the effect of the chosen method on the results of any subsequent analysis.

```{python}
from sklearn.impute import KNNImputer
imputer = KNNImputer()
df6_imputed = imputer.fit_transform(df6)
```

```{python}
df6_imputed
```

```{python}
df7 = df6.interpolate(inplace=True)
```

```{python}
df6.head()
```

```{python}
df6.isnull().sum()
```

```{python}
# Perform dimension reduction
pca = PCA(n_components=2)
principal_components = pca.fit_transform(df6)
principal_df6 = pd.DataFrame(data = principal_components, columns = ['PC1', 'PC2'])
```

```{python}
sns.regplot(x=principal_df6['PC1'], y=df6['quality'])
plt.show()
```

```{python}
sns.regplot(x=principal_df6['PC2'], y=df6['quality'])
plt.show()
```

```{python}
# Modeling
reg = LinearRegression().fit(principal_df6[['PC1', 'PC2']], df6['quality'])

# Evaluate the models
R_squared = reg.score(principal_df6[['PC1', 'PC2']], df6['quality'])
print("R-squared:", R_squared)

# Draw conclusions
print("The PCA and clustering methods were able to reduce the data to two principal components and group the observations into 3 clusters.")
print("The linear regression model using the two principal components as predictors was able to explain", R_squared, "of the variation in the target variable.")
```

## Step 5: Data Visualization {#sec-step5}

This is a crucial step in EDA, where the results of the analysis are visualized using various plots and charts. This helps in identifying patterns and relationships that may not be immediately apparent from the raw data.

```{python}
df6.quality.value_counts().plot(kind="bar")
plt.title("Value counts of the target variable")
plt.xlabel("Wine type")
plt.xticks(rotation=0)
plt.ylabel("Count")
plt.show()
```

```{python}
# Plot the distribution of wine quality
sns.countplot(x="quality", data=df6)
plt.show()
```

```{python}
plt.figure(figsize=(10,7))
plt.scatter(x="alcohol",y="fixed acidity",data =df6,marker= 'o',c="m")
plt.xlabel("alcohol",fontsize=15)
plt.ylabel("fixed_acidity",fontsize=15)
plt.show()
```

```{python}
sns.lmplot(x="alcohol",y="fixed acidity",data=df6)
plt.plot()
```

```{python}
plt.figure(figsize=(10,7))
plt.scatter(x="volatile acidity",y="alcohol",data =df6,marker= 'o',c="m")
plt.xlabel("volatile_acidity",fontsize=15)
plt.ylabel("alcohol",fontsize=15)
plt.show()
```

```{python}
sns.set()
sns.histplot(df6["quality"],bins=10)
plt.show()
```

```{python}
plt.figure(figsize=(10,7))
sns.regplot(x="citric acid",y="chlorides",data =df6,marker= 'o',color="m")
plt.show()
```

```{python}
corr_matrix = df6.corr()
sns.heatmap(corr_matrix)
plt.show()
```

```{python}
sns.set()
plt.figure(figsize=(20,10))
sns.boxplot(data=df6,palette="Set3")
plt.show()
```

There are some outliers. We can remove the outliers.

```{python}
lower_limit = df6["free sulfur dioxide"].mean() - 3*df6["free sulfur dioxide"].std()
upper_limit = df6["free sulfur dioxide"].mean() + 3*df6["free sulfur dioxide"].std()
```

```{python}
print(lower_limit,upper_limit)
```

```{python}
df7 = df6[(df6["free sulfur dioxide"] > lower_limit) & (df6["free sulfur dioxide"] < upper_limit)]
```

```{python}
df6.shape[0] - df7.shape[0]
```

```{python}
lower_limit = df7['total sulfur dioxide'].mean() - 3*df7['total sulfur dioxide'].std()
upper_limit = df7['total sulfur dioxide'].mean() + 3*df7['total sulfur dioxide'].std()
print(lower_limit,upper_limit)
```

```{python}
df8 = df7[(df7['total sulfur dioxide'] > lower_limit) & (df7['total sulfur dioxide'] < upper_limit)]
df8.head()
```

```{python}
df7.shape[0] - df8.shape[0]
```

```{python}
lower_limit = df8['residual sugar'].mean() - 3*df8['residual sugar'].std()
upper_limit = df8['residual sugar'].mean() + 3*df8['residual sugar'].std()
print(lower_limit,upper_limit)
```

```{python}
df9 = df8[(df8['residual sugar'] > lower_limit) & (df8['residual sugar'] < upper_limit)]
df9.head()
```

```{python}
df8.shape[0] - df9.shape[0]
```

```{python}
df9.isnull().sum()
```

## Step 6: Modeling {#sec-step6}

In the final stage of EDA, simple models such as linear regression or decision trees may be used to test the relationships identified in the previous steps.

### Feature Selection

```{python}
df9.quality.value_counts()
```

```{python}
df9.quality.value_counts(normalize = True)
```

```{python}
df9.head()
```

```{python}
quaity_mapping = { 3.0 : "Low", 4.0 : "Low", 5.0: "Medium", 6.0 : "Medium", 7.0 : "Medium", 8.0 : "High", 9.0 : "High"}
df9.loc[:, "quality"] = df9["quality"].map(quaity_mapping)
```

```{python}
df9.quality.value_counts()
```

```{python}
df9["quality"] = df9["quality"].astype("category")
df9["quality"] = df9["quality"].cat.codes
```

```{python}
df9.quality.value_counts()
```

Let us select the best features for the model. 

ExtraTreesClassifier is an ensemble method for classification that uses multiple decision trees to make predictions. It's a type of random forest algorithm, where each tree in the forest is built using a different random subset of the data and random subset of features. The final prediction is made by taking the average prediction from all the trees.

Once you've created the model instance, you can fit it to your training data and use it to make predictions on new data. To do this, you'll need to call the fit method on the model and pass in your training data.

```{python}
x = df9.drop("quality",axis=True)
y = df9["quality"]
model = ExtraTreesClassifier()
model.fit(x,y)
```

In machine learning, feature importance refers to the measure of how much a particular feature contributes to the prediction made by the model. In the case of decision tree based models, feature importances are typically calculated based on the reduction in impurity (such as Gini impurity or entropy) achieved when splitting a node in the tree based on that feature.

In the ExtraTreesClassifier model, the feature importances are stored in the feature_importances_ attribute. This attribute is an array where each element corresponds to the feature importance of a feature in the training data. The larger the value of the element, the more important the feature is considered by the model.

```{python}
print(model.feature_importances_)
```

We are printing the array of feature importances calculated by the ExtraTreesClassifier model. 

This information can be used to determine which features are the most important in predicting the target variable, and can be useful in feature selection and feature engineering.

```{python}
feat_importances = pd.Series(model.feature_importances_,index =x.columns)
feat_importances.nlargest(9).plot(kind="barh")
plt.show()
```

We are creating a pandas Series named feat_importances from the feature importances of the model (ExtraTreesClassifier). The index of the Series is set to the columns of the input feature data x, so each feature importance is associated with its corresponding feature.

Next, the code uses the nlargest method of the Series to select the 9 largest feature importances. Finally, the code plots these 9 largest feature importances as a horizontal bar plot using the plot method and specifying kind="barh". Finally, the code calls plt.show() to display the plot.

This code is used to visualize the feature importances of the ExtraTreesClassifier model in a bar plot. The bar plot provides a visual representation of the most important features in the data, and can be useful for interpreting the results of the model, as well as for feature selection.

### Model Selection

```{python}
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
```

```{python}
model_params  = {
    "svm" : {
        "model":SVC(gamma="auto"),
        "params":{
            'C' : [1,10,20],
            'kernel':["rbf"]
        }
    },
    
    "decision_tree":{
        "model": DecisionTreeClassifier(),
        "params":{
            'criterion':["entropy","gini"],
            "max_depth":[5,8,9]
        }
    },
    
    "random_forest":{
        "model": RandomForestClassifier(),
        "params":{
            "n_estimators":[1,5,10],
            "max_depth":[5,8,9]
        }
    },
    "naive_bayes":{
        "model": GaussianNB(),
        "params":{}
    },
    
    'logistic_regression' : {
        'model' : LogisticRegression(solver='liblinear',multi_class = 'auto'),
        'params': {
            "C" : [1,5,10]
        }
    }
    
}
```

```{python}
score=[]
for model_name,mp in model_params.items():
    clf = GridSearchCV(mp["model"],mp["params"],cv=8,return_train_score=False)
    clf.fit(x,y)
    score.append({
        "Model" : model_name,
        "Best_Score": clf.best_score_,
        "Best_Params": clf.best_params_
    })
```

```{python}
df10 = pd.DataFrame(score,columns=["Model","Best_Score","Best_Params"])
df10
```

We are getting 93% accuracy for SVM & Random Forest. 

```{python}
from sklearn.model_selection import cross_val_score
clf_svm = SVC(kernel="rbf",C=1)
scores = cross_val_score(clf_svm,x,y,cv=8,scoring="accuracy")
scores
```

```{python}
scores.mean()
```

### Prediction Model

The train_test_split function is a utility function provided by scikit-learn, a popular machine learning library in Python.

The function splits the data into training and testing sets, with the argument test_size specifying the proportion of the data that should be used for testing (in this case, 20%). The random_state argument is used to specify the random seed for the random number generator, ensuring that the split is reproducible.

The function returns four variables: x_train and y_train, which are the training data for the input features and target variable, respectively; and x_test and y_test, which are the testing data for the input features and target variable, respectively.

This split is a common and crucial step in the machine learning process, as it allows the model to be trained on a subset of the data and evaluated on a separate, held-out subset of the data. This helps to prevent overfitting, which is when the model becomes too closely tied to the training data, and to get a more accurate estimate of the model's performance on unseen data.

```{python}
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)
```

```{python}
clf_svm1 = SVC(kernel="rbf",C=1)
clf_svm1.fit(x_train,y_train)
```

The code is fitting a Support Vector Machine (SVM) classifier with a radial basis function (RBF) kernel and a regularization parameter C = 1. The SVM classifier is trained on the training data set x_train and y_train and the learned model is stored in the variable clf_svm1.

```{python}
y_pred = clf_svm1.predict(x_test)
```

```{python}
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(y_test,y_pred)
```

```{python}
accuracy
```

Compare the Real value and Predicted Value

```{python}
accuracy_dataframe = pd.DataFrame({"y_test": y_test, "y_pred": y_pred})
```

```{python}
accuracy_dataframe.head(20)
```


```{python}
y_pred = clf_svm1.predict(x_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

mismatch = (y_test != y_pred)
print("Mismatched samples:", mismatch.sum())

x_test_mismatch = x_test[mismatch]
y_test_mismatch = y_test[mismatch]
y_pred_mismatch = y_pred[mismatch]

print("Mismatched Actual vs Predicted:")
for i, (act, pred) in enumerate(zip(y_test_mismatch, y_pred_mismatch)):
    print(f"Sample {i}: Actual={act}, Predicted={pred}")
```

## Titanic Case {#sec-titanic}

```{python}
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the Titanic dataset
titanic = pd.read_csv("https://raw.githubusercontent.com/Dong2Yo/Dataset/main/titanic.csv")

# Get information about the dataset
print(titanic.info())

# Get the summary statistics of the dataset
print(titanic.describe())

# Plot the distribution of ages
sns.histplot(titanic["Age"].dropna())
plt.show()

# Plot the distribution of fares
sns.histplot(titanic["Fare"])
plt.show()

# Plot the relationship between age and fare
sns.jointplot(x="Age", y="Fare", data=titanic, kind="reg")
plt.show()

# Plot the relationship between class and survival
sns.countplot(x="Pclass", hue="Survived", data=titanic)
plt.show()

# Plot the relationship between gender and survival
sns.countplot(x="Sex", hue="Survived", data=titanic)
plt.show()
```

We can see that there is a strong relationship between class and survival, with passengers in higher class more likely to survive. We can also see that there is a relationship between gender and survival, with female passengers more likely to survive.

From these initial observations, we can further explore the data to answer additional questions, such as:

    How does the relationship between class and survival vary by gender?
    How does age affect survival for different classes and genders?
    What other factors might be related to survival on the Titanic?

**Grouping by multiple variables**: To understand the relationship between class, gender, and survival, we can group the data by these variables and calculate the survival rate for each group.

```{python}
# Group the data by class and gender and calculate the mean survival rate
grouped = titanic.groupby(["Pclass", "Sex"])["Survived"].mean()
print(grouped)
```

**Cross-tabulation**: To compare the survival rate between different groups, we can create a cross-tabulation (also known as a pivot table) that shows the number of survivors for each class and gender.

```{python}
# Create a cross-tabulation of class and gender by survival
ct = pd.crosstab(titanic["Pclass"], titanic["Sex"], values=titanic["Survived"], aggfunc="mean")
print(ct)
```

**Handling missing values**: In many datasets, missing values are a common problem. To handle missing values in the Titanic dataset, we can fill in the missing values with the mean or median value of the column.

```{python}
# Fill in the missing values in the Age column with the mean value
titanic["Age"].fillna(titanic["Age"].mean(), inplace=True)
```

**Plotting multiple variables**: To better understand the relationships between multiple variables, we can create a scatter plot matrix that plots all the variables against each other.

```{python}
# Create a scatter plot matrix of all the variables
sns.pairplot(titanic, hue="Survived")
plt.show()
```

Building models with the Titanic dataset can help us answer more advanced questions about survival on the ship. There are several machine learning algorithms that can be used for this task.

    Logistic Regression: Logistic regression is a simple and fast algorithm that can be used to predict binary outcomes, such as survival or death. In this case, we would use logistic regression to predict the survival of passengers on the Titanic based on their characteristics, such as age, gender, and class.

```{python}
# drop the name variable
titanic.drop(columns=["Name"], axis=1, inplace=True)
```

The axis parameter is set to 1 to indicate that you want to drop a column (as opposed to a row). By default, the drop method does not modify the original dataframe, but returns a new dataframe with the specified column dropped. To modify the original dataframe, you can use the inplace parameter and set it to True:

```{python}
titanic["Sex"] = titanic["Sex"].map({"male": 0, "female": 1})
```

This code maps the values "male" to 0 and "female" to 1, creating a new integer representation of the "Sex" column. You can then use this integer representation in your analysis and modeling, instead of the original string values. Note that the map method does not modify the original dataframe, but returns a new dataframe with the specified mapping applied. To modify the original dataframe, you can assign the result of the map method back to the original column:

```{python}
# Import the necessary libraries
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Prepare the data for modeling
X = titanic.drop(["Survived"], axis=1)
y = titanic["Survived"]

# Split the data into training and testing sets
X = titanic[["Pclass", "Sex", "Age"]]
y = titanic["Survived"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Train a logistic regression model
model1 = LogisticRegression()
model1.fit(X_train, y_train)

# Evaluate the model on the testing set
score = model1.score(X_test, y_test)
print("Logistic Regression Accuracy: {:.2f}%".format(score * 100))

# Plot the logistic regression model
sns.regplot(x=X_train["Age"], y=y_train, logistic=True)
plt.show()

sns.regplot(x=X_train["Pclass"], y=y_train, logistic=True)
plt.show()
```

    Decision Trees: Decision trees are a type of tree-based algorithm that can be used to model decision-making processes. In this case, we would use a decision tree to model the decision-making process that passengers on the Titanic went through to determine their survival.

```{python}
# Import the necessary libraries
from sklearn.tree import DecisionTreeClassifier

# Train a decision tree model
model2 = DecisionTreeClassifier()
model2.fit(X_train, y_train)

# Evaluate the model on the testing set
score = model2.score(X_test, y_test)
print("Decision Tree Accuracy: {:.2f}%".format(score * 100))
```

    Random Forest: Random forest is an ensemble learning algorithm that combines the outputs of multiple decision trees to produce a more accurate prediction. In this case, we would use a random forest to improve the accuracy of our model for predicting survival on the Titanic.

```{python}
# Import the necessary libraries
from sklearn.ensemble import RandomForestClassifier

# Train a random forest model
model3 = RandomForestClassifier()
model3.fit(X_train, y_train)

# Evaluate the model on the testing set
score = model3.score(X_test, y_test)
print("Random Forest Accuracy: {:.2f}%".format(score * 100))
```

Model building is just one part of the overall data analysis process. Once you have built your model, it's important to evaluate its performance and interpret the results.

    Cross-Validation: Cross-validation is a technique used to evaluate the performance of a model by dividing the data into multiple parts, training the model on one part and testing it on the others. This helps to reduce the risk of overfitting, where a model performs well on the training data but poorly on new data.

```{python}
# Import the necessary libraries
from sklearn.model_selection import cross_val_score

# Evaluate the model using cross-validation
scores1 = cross_val_score(model1, X, y, cv=5)
scores2 = cross_val_score(model2, X, y, cv=5)
scores3 = cross_val_score(model3, X, y, cv=5)
print(" Logistic Regression Accuracy: {:.2f}% (+/- {:.2f}%)".format(scores1.mean() * 100, scores1.std() * 2 * 100))
print(" Decison Tree Accuracy: {:.2f}% (+/- {:.2f}%)".format(scores2.mean() * 100, scores2.std() * 2 * 100))
print(" Random Forest Accuracy: {:.2f}% (+/- {:.2f}%)".format(scores3.mean() * 100, scores3.std() * 2 * 100))
```

    Confusion Matrix: A confusion matrix is a table that is used to evaluate the performance of a classification model by comparing the predicted labels to the true labels. The matrix shows the number of true positives, false positives, true negatives, and false negatives, and can be used to calculate metrics such as precision, recall, and F1-score.

```{python}
# Import the necessary libraries
from sklearn.metrics import confusion_matrix

# Predict the labels on the testing set
y_pred1 = model1.predict(X_test)
y_pred2 = model2.predict(X_test)
y_pred3 = model3.predict(X_test)

# Compute the confusion matrix
cm1 = confusion_matrix(y_test, y_pred1)
cm2 = confusion_matrix(y_test, y_pred2)
cm3 = confusion_matrix(y_test, y_pred3)

print("Logistic Regression")
print(cm1)
print("")

print("Decision Tree")
print(cm2)
print("")

print("Random Forest")
print(cm3)
```

    ROC Curve: A ROC (receiver operating characteristic) curve is a graph that is used to evaluate the performance of a binary classification model. The curve shows the trade-off between the true positive rate and false positive rate, and the area under the curve (AUC) can be used to compare the performance of different models.

```{python}
# Import the necessary libraries
from sklearn.metrics import roc_auc_score

# Predict the probabilities on the testing set
y_prob1 = model1.predict_proba(X_test)[:, 1]
y_prob2 = model2.predict_proba(X_test)[:, 1]
y_prob3 = model3.predict_proba(X_test)[:, 1]

# Compute the ROC AUC score
auc1 = roc_auc_score(y_test, y_prob1)
auc2 = roc_auc_score(y_test, y_prob2)
auc3 = roc_auc_score(y_test, y_prob3)

print("Logistic Regression ROC AUC: {:.2f}".format(auc1))
print("")
print("Decision Tree ROC AUC: {:.2f}".format(auc2))
print("")
print("Random Forest ROC AUC: {:.2f}".format(auc3))
```

    Feature Importance: Feature importance is a measure of how much each feature contributes to the prediction of the target variable. This can help you to understand which features are most important in your model, and can be used to identify features that are not useful or redundant.

```{python}
# Import the necessary libraries
from sklearn.ensemble import RandomForestClassifier

# Train a random forest classifier
model = RandomForestClassifier(n_estimators=100)
model.fit(X, y)

# Compute the feature importances
importances = model.feature_importances_

# Plot the feature importances
importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})
importance_df = importance_df.sort_values('Importance', ascending=False)
importance_df.plot(kind='bar', x='Feature', y='Importance')
```

    Learning Curve: A learning curve is a graph that shows the relationship between the training size and the training and testing error of a model. This can help you to understand if your model is overfitting or underfitting, and can be used to identify if you need to increase the size of your training data or if you need to use a more complex model.

```{python}
# Import the necessary libraries
from sklearn.model_selection import learning_curve

# Compute the learning curve
train_sizes, train_scores, test_scores = learning_curve(model, X, y)

# Plot the learning curve
plt.plot(train_sizes, train_scores.mean(axis=1), 'o-', label='Training Error')
plt.plot(train_sizes, test_scores.mean(axis=1), 'o-', label='Testing Error')
plt.xlabel('Training Size')
plt.ylabel('Error')
plt.legend()
```

## Absent from Work {#sec-absent}

### 0 Import and Read Data

The taset can be found at the following url-

https://archive.ics.uci.edu/ml/datasets/Absenteeism+at+work

The dataset consists of records of absenteeism at work from July 2007 to July 2010 at a courier company in Brazil. The dataset contains 740 number of instances and 21 number of attributes.

It was created by Andrea Martiniano, Ricardo Pinto Ferreira and Renato Jose Sassi.

Attribute information in the dataset is as follows:-

1. ID – represents individual identification ID

2. Reason for absence (ICD) – 

    Absences attested by the International Code of Diseases (ICD) stratified into 21 categories (I to XXI) as follows: 

    I Certain infectious and parasitic diseases
    
    II Neoplasms 
    
    III Diseases of the blood and blood-forming organs and certain disorders involving the immune mechanism 
    
    IV Endocrine, nutritional and metabolic diseases 
    
    V Mental and behavioural disorders 
    
    VI Diseases of the nervous system
    
    VII Diseases of the eye and adnexa
    
    VIII Diseases of the ear and mastoid process 
    
    IX Diseases of the circulatory system 
    
    X Diseases of the respiratory system
    
    XI Diseases of the digestive system
    
    XII Diseases of the skin and subcutaneous tissue 
    
    XIII Diseases of the musculoskeletal system and connective tissue
    
    XIV Diseases of the genitourinary system
    
    XV Pregnancy, childbirth and the puerperium
   
    XVI Certain conditions originating in the perinatal period
    
    XVII Congenital malformations, deformations and chromosomal abnormalities
    
    XVIII Symptoms, signs and abnormal clinical and laboratory findings, not elsewhere classified
    
    XIX Injury, poisoning and certain other consequences of external causes
    
    XX External causes of morbidity and mortality 
    
    XXI Factors influencing health status and contact with health services. 

    And 7 categories without (CID) patient follow-up (22), medical consultation (23), blood donation (24), laboratory examination (25), unjustified absence (26), physiotherapy (27), dental consultation (28).

3.	Month of absence 
4. Day of the week (Monday (2), Tuesday (3), Wednesday (4), Thursday (5), Friday (6)) 
5. Seasons (summer (1), autumn (2), winter (3), spring (4)) 
6. Transportation expense 
7. Distance from Residence to Work (kilometers) 
8. Service time 
9. Age 
10. Work load Average/day 
11. Hit target 
12. Disciplinary failure (yes=1; no=0) 
13. Education (high school (1), graduate (2), postgraduate (3), master and doctor (4)) 
14. Son (number of children) 
15. Social drinker (yes=1; no=0) 
16. Social smoker (yes=1; no=0) 
17. Pet (number of pet) 
18. Weight 
19. Height 
20. Body mass index 
21. Absenteeism time in hours (target) 


```{python}
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scipy.stats as st
%matplotlib inline
```

```{python}
sns.set(style="whitegrid")
```

We can import the dataset using the usual **read_csv()** function as follows:-

```{python}
data = "https://raw.githubusercontent.com/Dong2Yo/Dataset/main/Absenteeism_at_work_AAA/Absenteeism_at_work.csv"

df = pd.read_csv(data, sep=";")
```

Generally, in the csv file the values are separated by a comma. So, there is no need to use the sep parameter which describes how the values are separated. 

In this case, the values are separated by semicolon (;). So, we can use the **sep = ";"** parameter to denote that the values are separated by semicolon. 

Overview of the dataset

Now, we should get to know our data. We should know its dimensions, structure and column data types. 

**df.shape attribute**

```{python}
print(df.shape)
```

**df.columns attribute**

```{python}
print(df.columns)
```

**df.head() and df.tail() methods**

```{python}
df.head()
```

```{python}
df.tail()
```

**df.info() method**

```{python}
df.info()
```

The data types of several columns like "Month of absence", "Transportation expense", "Distance from Residence to work", "Hit target", "Education", "Weight", "Height", "Body mass index" and "Absenteeism time in hours" should be real or float. But, the above cell shows that they have integer data types. So, we need to convert their data types into float.

### 1 Clean Data

```{python}
df[["Month of absence","Transportation expense","Distance from Residence to Work", "Hit target","Education","Weight","Height",
"Body mass index","Absenteeism time in hours"]]=df[["Month of absence","Transportation expense","Distance from Residence to Work",
"Hit target","Education","Weight","Height","Body mass index","Absenteeism time in hours"]].astype(float)
```

```{python}
df.info()
```

We can see that the data types of modified columns are float64. Now, all the columns are appropriate data types.

**drop redundant columns**


There are two columns `ID` and `Pet` which have no correlation with the target variable `Absenteeism time in hours`. So, we should drop these columns. 

```{python}
df.drop(['ID','Pet'], axis = 1, inplace=True)
```

**df.describe() method**

```{python}
print(df.describe())
```

We can see that the minimum value of **Month of absence** is zero. It cannot be zero. The minimum value should be one. So, we need to replace zero by one.

```{python}
df['Month of absence'].replace(0,1,inplace=True)
```

```{python}
print(df.describe())
```

**Check for anomalies in the dataset**

<u>Check for missing numerical values</u>


The first step is to check for any missing values in the dataset. We can check for missing values in the dataset using
the `df.isnull().sum()` command. This command returns the total number of missing values in each column in the dataset.


If we want to check for 'NA' values in a particular column in the dataframe, then we should use the following command
`pd.isna(df['col_name'])`.

```{python}
df.isnull().sum()
```

<u>Check with ASSERT statement</u>

We should confirm that our dataset has no missing values. We can write an **assert statement** to verify this. We can use an **assert statement** to programmatically check that no missing, unexpected 0 or negative values are present. This gives us confidence that our code is running properly.


Assert statement will return nothing if the value being tested is true and will throw an AssertionError if the value is false.

Asserts

•	assert 1 == 1   (return Nothing if the value is True)

•	assert 1 == 2   (return AssertionError if the value is False)

```{python}
#assert that there are no missing values in the dataframe

assert pd.notnull(df).all().all()
```

```{python}
#assert all values are greater than or equal to 0

assert (df >= 0).all().all()
```

The above two commands do not throw any error. Hence, it is confirmed that there are no missing or negative values in the dataset. All the values are greater than or equal to zero.

### 2 Univariate Analysis

<u>Measures of central tendency and dispersion</u>


**Central tendency** means a central value which describe a probability distribution. It may also be called a center or location of the distribution. The most common measures of central tendency are the arithmetic mean, the median and the mode. The most common measure of central tendency is the mean. For skewed distribution or when there is concern about outliers, the median
may be preferred. So, median is more robust measure than the mean.

**Dispersion** is an indicator of how far away from the center, we can find the data values. The most common measures of dispersion are variance, standard deviation and interquartile range(IQR). Variance is the standard measure of spread. The 
standard deviation is the square root of the variance. The variance and standard deviation are two useful measures of
spread. 

A third measure of spread is the interquartile range (IQR). The IQR is calculated using the boundaries of data situated between the 1st and the 3rd quartiles. So, IQR can be calculated as IQR = Q3 - Q1. It is a robust measure of spread.

```{python}
print(df['Absenteeism time in hours'].describe())
```

**Interpretation**

The count, min and max values represent the number of counts, minimum and maximum values of the target variable `Absenteeism time in hours`. 

The measures of central tendency are given by the mean(6.924324) and median(50% value-3.00). 

The measure of dispersion is given by the standard deviation given by std(13.330998).

The 25%, 50% and 75% values show the corresponding percentiles. 50th percentile denote the median of the distribution.

The IQR is the difference between 75th and 25th percentiles. Hence, IQR = 8.00 - 2.00 = 6.00

<u>Measures of shape</u>


We have looked at the measures of central tendency of the data (mean and median) and spread of the data (standard deviation(std), interquartile range, minimum (min) and maximum (max) values. These quantities can only be used for quantitative 
variables not for categorical variables.

Now, we will take a look at measures of shape of distribution. There are two statistical measures that can tell us about the shape of the distribution. These measures are **skewness** and **kurtosis**. These measures can be used to convey information about the shape of the distribution of the dataset.

First, we will look at skewness and later we get to know about kurtosis.

<u>Skewness</u>


Skewness is a measure of a distribution's symmetry or more precisely lack of symmetry. It is used to mean the absence of symmetry from the mean of the dataset. It is a characteristic of the deviation from the mean. It is used to indicate the 
shape of the distribution of data.



<u>Negative skewness</u>

Negative values for skewness indicate negative skewness. In this case, the data are skewed or tail to left. By skewed left, 
we mean that the left tail is long relative to the right tail. The data values may extend further to the left but concentrated
in the right. So, there is a long tail and distortion is caused by extremely small values which pull the mean downward so that it is less than the median. Hence, in this case

**Mean < Median < Mode**



<u>Zero skewness</u>

Zero skewness means skewness value of zero. It means the dataset is symmetrical. A data set is symmetrical if it looks the 
same to the left and right to the center point. The dataset looks bell shaped or symmetrical. A perfectly symmetrical data set will have a skewness of zero. So, the normal distribution which is perfectly symmetrical has a skewness of 0. So, in this case

**Mean = Median = Mode**



<u>Positive skewness</u>

Positive values for skewness indicate positive skewness. The dataset are skewed or tail to right. By skewed right, we mean that the right tail is long relative to the left tail. The data values are concentrated in the right. So, there is a long tail to the 
right that is caused by extremely large values which pull the mean upward so that it is greater than the median. So, we have

**Mean > Median > Mode**



<u>Reference range on skewness values</u>

The rule of thumb for skewness values are:

If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.

If the skewness is between -1 and – 0.5 or between 0.5 and 1, the data are moderately skewed.

If the skewness is less than -1 or greater than 1, the data are highly skewed.

```{python}
df['Absenteeism time in hours'].skew()
```

The skewness of our target variable `Absenteeism time in hours` comes out to be greater than +1. So, we can conclude that the 
target variable is highly positively skewed. 

We can confirm this by plotting a Seaborn histplot diagram.

```{python}
plt.figure(figsize= (10,8))
sns.displot(df["Absenteeism time in hours"])
plt.title("Distribution of Absenteeism time in hours")
plt.show()
```

The above plot confirms that the target variable `Absenteeism time in hours` is highly positively skewed.

<u>Kurtosis</u>

Kurtosis is the degree of peakedness of a distribution. 

Data sets with high kurtosis tend to have a distinct peak near the mean, decline rather rapidly and have heavy tails.

Data sets with low kurtosis tend to have a flat top near the mean rather than a sharp peak. 


**Reference range for kurtosis**

The reference standard is a normal distribution, which has a kurtosis of 3. Often, **excess kurtosis** is presented instead of kurtosis, where **excess kurtosis** is simply **kurtosis - 3**. 


**Mesokurtic curve**

A normal distribution has kurtosis exactly 3 (**excess kurtosis** exactly 0). Any distribution with kurtosis ≈3 (excess ≈ 0) is called **mesokurtic**.


**Platykurtic curve**

A distribution with kurtosis < 3 (**excess kurtosis** < 0) is called **platykurtic**. As compared to a normal distribution, its 
central peak is lower and broader, and its tails are shorter and thinner.


**Leptokurtic curve**

A distribution with kurtosis > 3 (**excess kurtosis** > 0) is called **leptokurtic**. As compared to a normal distribution, its central peak is higher and sharper, and its tails are longer and fatter.

```{python}
df['Absenteeism time in hours'].kurt()
```

The kurtosis value of the `Absenteeism time in hours` is much much greater than 3. So, we can conclude that the distribution 
curve is a **Leptokurtic curve**. Its central peak is higher and sharper and its tails are longer and fatter.

<u>Findings of univariate analysis</u>


Findings of univariate analysis are as follows:-


•	The target variable `Absenteeism time in hours` is highly positively skewed.

•	Its distribution curve is a **Leptokurtic curve**. Its central peak is higher and sharper and its tails are longer and fatter.

### 3 Multivariate Analysis

<u>Examine relationship between target variable and categorical attributes</u>


In the dataset, we have several categorical attributes like `Seasons`, `Education`, `Social drinker` and `Social smoker`. 
We can explore the relationship between these categorical attributes and target variable.

`Seasons` is a categorical attribute. We can find out what categories exist and how many values belong to each category using the `value_counts()` method.

```{python}
df['Seasons'].value_counts()
```

```{python}
bar_colors = ['green', 'grey', 'brown', 'orange']
df['Seasons'].value_counts().plot(kind = 'bar', color=bar_colors, figsize=(10,5))
plt.title('Absenteeism time in hours in various seasons')
plt.xlabel('Seasons')
plt.ylabel('Absenteeism time in hours')
plt.legend()
plt.show()
```

**Conclusion**

`Seasons` attribute contain 4 data values as 1, 2, 3 and 4. These values represent 4 different seasons in a year which are coded as summer = 1, autumn = 2, winter = 3, spring = 4. So, we can conclude that spring contains highest number of `Absenteeism time in hours`.

Similarly, `Education`, `Social drinker` and `Social smoker` are also categorical attributes. We can visualize their frequency distribution using the value_counts() method and visualize them as follows:-

```{python}
df['Education'].value_counts()
```

```{python}
df['Education'].value_counts().plot(kind = 'bar', figsize=(10,5))
plt.title('Absenteeism time in hours in various seasons')
plt.xlabel('Education')
plt.ylabel('Absenteeism time in hours')
plt.legend()
plt.show()
```

**Conclusion**

Education categorical attribute is coded as 1.0, 2.0, 3.0 and 4.0 which stands for different categories. The categories are 
high school (1), graduate (2), postgraduate (3), master and doctor (4). We can see that the high school category consists of highest number of `Absenteeism time in hours`.

```{python}
df['Social drinker'].value_counts()
```

```{python}
df['Social drinker'].value_counts().plot(kind = 'bar', figsize=(10,5))
plt.title('Absenteeism time in hours in various seasons')
plt.xlabel('Social drinker')
plt.ylabel('Absenteeism time in hours')
plt.show()
```

**Conclusion**

Social drinker consists of two categories - (yes=1; no=0). From the graph, we can conclude that `Social drinker` have higher 
number of `Absenteeism time in hours`.

```{python}
df['Social smoker'].value_counts()
```

```{python}
df['Social smoker'].value_counts().plot(kind = 'bar', figsize=(10,5))
plt.title('Absenteeism time in hours in various seasons')
plt.xlabel('Social smoker')
plt.ylabel('Absenteeism time in hours')
plt.show()
```

**Conclusion**

Social smoker consists of two categories - (yes=1; no=0). From the graph, we can conclude that `Social smoker` have lesser 
number of `Absenteeism time in hours`.

<u>Initial Findings of multivariate analysis</u>

Findings of multivariate analysis are as follows:-

•	The spring season contains highest number of `Absenteeism time in hours`.

•	The high school category consists of highest number of `Absenteeism time in hours`.

•	The `Social drinker` category have higher number of `Absenteeism time in hours`.

•	The `Social smoker` category have lesser number of `Absenteeism time in hours`.

**Estimating correlation coefficients**

Our dataset is very small. So, we can compute the standard correlation coefficient (also called Pearson's r) between every pair of attributes. We can compute it using the `df.corr()` method.

```{python}
correlation = df.corr()
correlation
```

Our target variable is `Absenteeism time in hours`. So, we should check how each attribute correlates with the `Absenteeism time
in hours` variable. 

```{python}
correlation['Absenteeism time in hours'].sort_values(ascending=False)
```

**Interpretation of correlation coefficient**

The correlation coefficient ranges from -1 to +1. 

When it is close to +1, this signifies that there is a strong positive correlation. So, we can see that there is a small positive correlation between `Absenteeism time in hours` and `Height`. 


When it is clsoe to -1, it means that there is a strong negative correlation. So, there is a small negative correlation between `Absenteeism time in hours` and `Reason for absence`.


When it is close to 0, it means that there is no correlation. So, there is no correlation between `Absenteeism time in hours` and `Seasons`.

<u>Discover patterns and relationships</u>


An important step in EDA is to discover patterns and relationships between variables in the dataset. We will use the following
graphs and plots to explore the patterns and relationships in the dataset.



### 5 Visualize Data

**Correlation Heat Map**

```{python}
plt.figure(figsize=(16,12))
plt.title('Correlation of Attributes with Absenteeism time in hours')
a = sns.heatmap(correlation, square=True, annot=True, fmt='.2f', linecolor='white')
a.set_xticklabels(a.get_xticklabels(), rotation=90)
a.set_yticklabels(a.get_yticklabels(), rotation=30)           
plt.show()
```

From the above correlation heat map, we can conclude that :-

1. `Month of absence` and `Seasons` are positively correlated (correlation coefficient = 0.41).

2. `Body mass index` and `Service time` are positively correlated (correlation coefficient = 0.50).

3. Smilarly, `Body mass index` and `Age` are positively correlated (correlation coefficient = 0.47).

4. Also, `Body mass index` and `Weight` are highly positively correlated (correlation coefficient = 0.90).


**Pair Plot**

```{python}
num_var = ['Transportation expense', 'Distance from Residence to Work', 'Service time', 'Age', 'Work load Average/day ', 
           'Absenteeism time in hours']
sns.pairplot(df[num_var], kind='scatter', diag_kind='hist')
plt.show()
```

**Conclusion**

1. The above pair plot confirms that there is strong positive correlation between `Service time` and `Age`.

2. Similarly, `Distance from Residence to Work` and `Transportation expense` are positively correlated. 

**Scatter Plot of Absenteeism time in hours and other variuables**

```{python}
sns.lmplot(x='Absenteeism time in hours', y='Height', data=df)
plt.show()
```

**Conclusion**

The above scatter-plot shows that there is a mildly positive correlation between `Absenteeism time in hours` and `Height`. 
Majority of data values lie below the fitted regression line.

```{python}
sns.lmplot(x='Absenteeism time in hours', y='Son', data=df)
plt.show()
```

**Conclusion**

The above scatter-plot shows that there is a weak correlation between `Absenteeism time in hours` and `Son`. 

```{python}
sns.lmplot(x='Body mass index', y='Service time', data=df)
plt.show()
```

**Conclusion**

The above scatter-plot shows that there is a strong positive correlation between `Body mass index` and `Service time`. 
Approximately, half of the data values lie below the fitted regression line and half of the values lie above it.

**Box Plot of Absenteeism time in hours and age**

```{python}
plt.rcParams['figure.figsize']=(15,5)
ax = sns.boxplot(x='Age', y='Absenteeism time in hours', data=df)
```

**Conclusion**

The above box-plot confirms that the people aged 34 or 58 have highest number of `Absenteeism time in hours`.

```{python}
plt.rcParams['figure.figsize']=(15,5)
ax = sns.boxplot(x='Son', y='Absenteeism time in hours', data=df)
```

**Conclusion**

The people who have 2 sons have highest number of `Absenteeism time in hours`.

**Findings of multivariate analysis**


•	`Month of absence` and `Seasons` are positively correlated (correlation coefficient = 0.41).


•	`Body mass index` and `Service time` are positively correlated (correlation coefficient = 0.50).


•	Smilarly, `Body mass index` and `Age` are positively correlated (correlation coefficient = 0.47).


•	Also, `Body mass index` and `Weight` are highly positively correlated (correlation coefficient = 0.90).


•	The pair plot confirms that there is strong positive correlation between `Service time` and `Age`.


•	Similarly, `Distance from Residence to Work` and `Transportation expense` are positively correlated.


•	There is a mildly positive correlation between `Absenteeism time in hours` and `Height`. Majority of data values lie below 
    the fitted regression line.
    

•	There is a weak correlation between `Absenteeism time in hours` and `Son`.


•	There is a strong positive correlation between `Body mass index` and `Service time`. Approximately, half of the data values 
    lie below the fitted regression line and half of the values lie above it.


•	The people aged 34 or 58 have highest number of `Absenteeism time in hours`.


•	The people who have 2 sons have highest number of `Absenteeism time in hours`.

### 6 Prediction Model

<u>A. Import the necessary libraries</u>

```{python}
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
```

<u>B. Preprocess the data</u>

```{python}
# Split into features and target
X = df.drop('Absenteeism time in hours', axis=1)
y = df['Absenteeism time in hours']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Scale the data
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
```

<u>C. Train and evaluate multiple algorithms</u>

```{python}
# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred_log_reg = log_reg.predict(X_test)
```

```{python}
# Decision Tree
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
y_pred_dt = dt.predict(X_test)

```

```{python}
# Random Forest
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

```

<u>D. Compare Performances</u>

We can use various performance metrics such as accuracy, precision, recall, F1 score, and others.

    Accuracy: It is the proportion of correct predictions made by the model.

    Precision: It is the number of true positive predictions out of all positive predictions made by the model.

    Recall: It is the number of true positive predictions out of all actual positive instances.

    F1 Score: It is the harmonic mean of precision and recall.

We can calculate these metrics for each of the models, and based on the results, we can choose the one with the best performance. 

However, it's important to note that a model with a high accuracy score may not always be the best choice as it may be biased towards the majority class. Hence, it's advisable to consider other metrics as well to get a comprehensive understanding of the model's performance.

```{python}
# calculate the performance metrics
acc_log_reg = accuracy_score(y_test, y_pred_log_reg)
prec_log_reg = precision_score(y_test, y_pred_log_reg, average='weighted')
rec_log_reg = recall_score(y_test, y_pred_log_reg, average='weighted')
f1_log_reg = f1_score(y_test, y_pred_log_reg, average='weighted')

acc_dt = accuracy_score(y_test, y_pred_dt)
prec_dt = precision_score(y_test, y_pred_dt, average='weighted')
rec_dt = recall_score(y_test, y_pred_dt, average='weighted')
f1_dt = f1_score(y_test, y_pred_dt, average='weighted')

acc_rf = accuracy_score(y_test, y_pred_rf)
prec_rf = precision_score(y_test, y_pred_rf, average='weighted')
rec_rf = recall_score(y_test, y_pred_rf, average='weighted')
f1_rf = f1_score(y_test, y_pred_rf, average='weighted')

# Create a table to compare the performance of each algorithm
results = pd.DataFrame({
    "Algorithm": ["Logistic Regression", "Decision Tree", "Random Forest"],
    "Accuracy": [acc_log_reg, acc_dt, acc_rf],
    "Precision": [prec_log_reg, prec_dt, prec_rf],
    "Recall": [rec_log_reg, rec_dt, rec_rf],
    "F1-Score": [f1_log_reg, f1_dt, f1_rf]
})

# Print the results
print(results)
```

## Resources {#sec-resources}

It's important to note that EDA is an iterative process and the steps may be repeated several times until a clear understanding of the data is obtained. The goal of EDA is to gain insights into the data and generate hypotheses for further investigation, rather than to develop a final model.

There are many resources available to learn Exploratory Data Analysis (EDA) in Python. Some of the most useful ones are:

    Books: "Python for Data Analysis" by Wes McKinney and "Data Science from Scratch" by Joel Grus are two popular books that cover the basics of EDA in Python.

    Online courses: Coursera, Udemy, and EdX offer many online courses that cover EDA in Python, including "Applied Plotting, Charting & Data Representation in Python" and "Data Visualization with Python."

    Documentation and tutorials: The official documentation and tutorials for libraries such as Pandas, Matplotlib, and Seaborn provide in-depth explanations and examples of how to perform EDA in Python.

    YouTube: There are many YouTube tutorials and presentations on EDA in Python that can help you get started with the basics.

It's also a good idea to practice EDA on real datasets as you learn. You can find many datasets on websites such as Kaggle, UCI Machine Learning Repository, and FiveThirtyEight.


